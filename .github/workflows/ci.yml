name: CI - Quality Gates & Testing

on:
  pull_request:
    branches: [ main ]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  # =============================================================================
  # STATIC ANALYSIS (Fast feedback, parallel execution)
  # =============================================================================
  
  static-analysis:
    name: Static Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Install Rust toolchain
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          components: rustfmt, clippy
          override: true
          
      - name: Setup Rust cache
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "rust-static-analysis"
          cache-all-crates: "true"
          
      - name: Check Rust formatting
        run: cargo fmt --all -- --check
        
      - name: Run Clippy
        run: cargo clippy --all-targets --all-features -- -D warnings
        
      - name: Security audit with enhanced reporting
        run: |
          cargo install --version 0.21.2 cargo-audit
          
          echo "Running Rust security audit..."
          if cargo audit --format json > audit-results.json 2>&1; then
            echo "âœ… No security vulnerabilities found"
          else
            echo "âš ï¸ Security vulnerabilities detected:"
            if [ -f audit-results.json ]; then
              # Parse and display vulnerabilities in readable format
              cat audit-results.json | jq -r '.vulnerabilities[]? | "- \(.advisory.title) (\(.advisory.id)) - Severity: \(.advisory.informational // "Unknown")"' 2>/dev/null || cat audit-results.json
            fi
            
            # Don't fail the build for informational advisories, but fail for actual vulnerabilities
            HIGH_SEVERITY=$(cat audit-results.json | jq -r '.vulnerabilities[]? | select(.advisory.informational != true) | .advisory.id' 2>/dev/null | wc -l)
            if [ "$HIGH_SEVERITY" -gt 0 ]; then
              echo "âŒ Found $HIGH_SEVERITY high-severity vulnerabilities - failing build"
              exit 1
            else
              echo "â„¹ï¸ Only informational advisories found - continuing build"
            fi
          fi

  frontend-static-analysis:
    name: Frontend Static Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 10
    defaults:
      run:
        working-directory: apps/web
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Install pnpm
        uses: pnpm/action-setup@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'pnpm'
        
      - name: Install dependencies
        run: pnpm install --frozen-lockfile
        
      - name: Lint
        run: pnpm lint
        
      - name: Type check
        run: pnpm type-check
        
      - name: Enhanced security audit
        run: |
          echo "Running frontend security audit..."
          
          # Run audit and capture results
          if pnpm audit --json > audit-results.json 2>&1; then
            echo "âœ… No high-severity vulnerabilities found"
          else
            echo "âš ï¸ Security issues detected in frontend dependencies:"
            
            # Display readable audit summary
            if [ -f audit-results.json ]; then
              HIGH_VULNS=$(cat audit-results.json | jq -r '.advisories[]? | select(.severity == "high" or .severity == "critical") | "- \(.title) (\(.severity))"' 2>/dev/null || echo "Could not parse audit results")
              
              if [ -n "$HIGH_VULNS" ] && [ "$HIGH_VULNS" != "Could not parse audit results" ]; then
                echo "High/Critical vulnerabilities:"
                echo "$HIGH_VULNS"
                echo "âŒ High-severity vulnerabilities found - failing build"
                exit 1
              else
                echo "â„¹ï¸ Only low/moderate severity issues found - review recommended but not blocking"
              fi
            fi
          fi

  # =============================================================================
  # UNIT TESTS (Fast, comprehensive coverage)
  # =============================================================================
  
  backend-unit-tests:
    name: Backend Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: static-analysis
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Need full history for smart test detection
        
      - name: Install Rust toolchain
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          override: true
          
      - name: Setup Rust cache
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "rust-unit-tests"
          cache-all-crates: "true"
          
      - name: Install jq for smart test runner
        run: sudo apt-get update && sudo apt-get install -y jq
        
      - name: Run smart unit tests (robust with fallback)
        run: |
          # Make smart test runner executable
          chmod +x scripts/smart-test-runner.sh
          
          # Try smart test execution first with comprehensive error handling
          SMART_TEST_FAILED=false
          if ! ./scripts/smart-test-runner.sh --base-ref origin/main 2>&1 | tee smart_test_output.log; then
            SMART_TEST_FAILED=true
            SMART_EXIT_CODE=${PIPESTATUS[0]}
            
            echo "âŒ Smart test execution failed (exit code: $SMART_EXIT_CODE)"
            echo "Smart test runner output (last 50 lines):"
            tail -50 smart_test_output.log
            echo ""
            
            echo "ðŸ”„ Attempting fallback to full test suite..."
            if cargo test --lib --workspace --verbose 2>&1 | tee fallback_test_output.log; then
              echo "âœ… Fallback test execution passed!"
              echo "âš ï¸  Note: Smart test runner failed, but all tests pass."
              echo "âš ï¸  Smart test runner should be investigated and fixed."
            else
              FALLBACK_EXIT_CODE=${PIPESTATUS[0]}
              echo "âŒ Fallback test execution also failed! (exit code: $FALLBACK_EXIT_CODE)"
              echo "Fallback test output (last 30 lines):"
              tail -30 fallback_test_output.log
              echo ""
              echo "ðŸ’¥ Both smart testing and fallback failed!"
              exit 1
            fi
          else
            echo "âœ… Smart test execution completed successfully"
          fi
          
          # Cleanup
          rm -f smart_test_output.log fallback_test_output.log
          
      - name: Generate coverage report
        run: |
          curl -s https://api.github.com/repos/xd009642/tarpaulin/releases/latest \
            | grep "browser_download_url.*x86_64-unknown-linux-musl.tar.gz" \
            | cut -d : -f 2,3 \
            | tr -d \" \
            | wget -qi - -O tarpaulin.tar.gz
          tar -xzf tarpaulin.tar.gz
          sudo mv cargo-tarpaulin /usr/local/bin/
          cargo tarpaulin \
            --lib \
            --workspace \
            --timeout 120 \
            --out xml \
            --output-dir ./coverage
            
      - name: Check coverage threshold
        run: |
          COVERAGE=$(grep -oP 'line-rate="\K[^"]*' coverage/tarpaulin-report.xml | head -1)
          COVERAGE_PERCENT=$(echo "$COVERAGE * 100" | bc -l | cut -d'.' -f1)
          echo "Coverage: ${COVERAGE_PERCENT}%"
          if [ "$COVERAGE_PERCENT" -lt 85 ]; then
            echo "âŒ Coverage ${COVERAGE_PERCENT}% is below required 85%"
            exit 1
          fi
          echo "âœ… Coverage requirement met: ${COVERAGE_PERCENT}%"
          
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage/tarpaulin-report.xml
          flags: backend
          name: backend-coverage
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  frontend-unit-tests:
    name: Frontend Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    defaults:
      run:
        working-directory: apps/web
    needs: frontend-static-analysis
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Install pnpm
        uses: pnpm/action-setup@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'pnpm'
        
      - name: Install dependencies
        run: pnpm install --frozen-lockfile
        
      - name: Run unit tests with coverage
        run: pnpm test:coverage
        
      - name: Check coverage threshold
        run: |
          COVERAGE=$(grep -oP '"pct":[^,]*' coverage/coverage-summary.json | head -1 | cut -d':' -f2)
          if [ "$COVERAGE" -lt 80 ]; then
            echo "âŒ Frontend coverage $COVERAGE% is below required 80%"
            exit 1
          fi
          echo "âœ… Frontend coverage requirement met: $COVERAGE%"
          
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./apps/web/coverage/lcov.info
          flags: frontend
          name: frontend-coverage
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  # =============================================================================
  # INTEGRATION TESTS (Database, inter-service communication)
  # =============================================================================
  
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [backend-unit-tests, frontend-unit-tests]
    
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_PASSWORD: password
          POSTGRES_USER: postgres
          POSTGRES_DB: gamalan_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
          
      qdrant:
        image: qdrant/qdrant:v1.8.1
        ports:
          - 6333:6333
        options: >-
          --health-cmd "wget --no-verbose --tries=1 --spider http://localhost:6333/health || exit 0"
          --health-interval 15s
          --health-timeout 10s
          --health-retries 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Install Rust toolchain
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          override: true
          
      - name: Setup Rust cache
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "rust-integration-tests"
          cache-all-crates: "true"
          
      - name: Run database migrations
        run: |
          cargo install sqlx-cli --no-default-features --features "postgres,rustls"
          # Run migrations for all services
          find services -name "migrations" -type d | while read -r migration_dir; do
            service=$(basename $(dirname $migration_dir))
            echo "Running migrations for $service"
            DATABASE_URL="postgres://postgres:password@localhost:5432/gamalan_test" \
              sqlx migrate run --source "$migration_dir" --ignore-missing
          done
        env:
          DATABASE_URL: postgres://postgres:password@localhost:5432/gamalan_test
          
      - name: Run integration tests
        run: |
          cargo test --test '*' --workspace --verbose
        env:
          DATABASE_URL: postgres://postgres:password@localhost:5432/gamalan_test
          QDRANT_URL: http://localhost:6333
          TEST_LOG: debug

  # =============================================================================
  # CONTRACT TESTS (OpenAPI validation)
  # =============================================================================
  
  contract-tests:
    name: Contract Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: integration-tests
    
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_PASSWORD: password
          POSTGRES_USER: postgres
          POSTGRES_DB: gamalan_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Install Rust toolchain
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          override: true
          
      - name: Setup Rust cache
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "rust-contract-tests"
          cache-all-crates: "true"
          
      - name: Setup Node.js for OpenAPI tools
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          
      - name: Install OpenAPI tools
        run: |
          set -e  # Exit on any error
          
          echo "Installing OpenAPI validation tools..."
          
          # Clean npm cache to avoid potential corruption
          npm cache clean --force
          
          # Install swagger-cli for basic OpenAPI validation
          echo "Installing @apidevtools/swagger-cli..."
          if ! npm install -g @apidevtools/swagger-cli; then
            echo "âŒ Failed to install swagger-cli via npm"
            exit 1
          fi
          
          # Install spectral via npm with explicit version for reliability
          echo "Installing @stoplight/spectral-cli..."
          if ! npm install -g @stoplight/spectral-cli@6.11.0; then
            echo "âŒ Failed to install spectral-cli via npm"
            echo "Attempting alternative installation method..."
            
            # Fallback: try installing without specific version
            if ! npm install -g @stoplight/spectral-cli; then
              echo "âŒ All spectral installation methods failed"
              exit 1
            fi
          fi
          
          # Wait a moment for npm to finalize installations
          sleep 2
          
          # Verify installations with detailed diagnostics
          echo "Verifying tool installations..."
          
          echo "swagger-cli location and version:"
          which swagger-cli || (echo "âŒ swagger-cli not in PATH" && exit 1)
          swagger-cli --version || (echo "âŒ swagger-cli not working" && exit 1)
          
          echo "spectral location and version:"
          which spectral || (echo "âŒ spectral not in PATH" && exit 1)
          
          # Additional spectral verification - check if it's a proper binary
          if [ ! -x "$(which spectral)" ]; then
            echo "âŒ spectral binary is not executable"
            ls -la $(which spectral)
            head -5 $(which spectral)
            exit 1
          fi
          
          # Test spectral command
          if ! spectral --version; then
            echo "âŒ spectral command failed"
            echo "spectral file contents:"
            head -10 $(which spectral)
            exit 1
          fi
          
          echo "âœ… All OpenAPI tools installed and verified successfully"
          echo "Installed tools:"
          echo "  - swagger-cli: $(swagger-cli --version)"
          echo "  - spectral: $(spectral --version)"
          
      - name: Validate OpenAPI specifications
        run: |
          set -e  # Exit on any error
          
          # Re-verify tools are working before using them
          echo "Pre-validation tool check..."
          swagger-cli --version || (echo "âŒ swagger-cli not available" && exit 1)
          spectral --version || (echo "âŒ spectral not available" && exit 1)
          
          # Check if we have any OpenAPI specs to validate
          OPENAPI_SPECS=$(find services -name "openapi.yaml" -o -name "openapi.yml")
          
          if [ -z "$OPENAPI_SPECS" ]; then
            echo "No OpenAPI specifications found to validate"
            echo "Expected location: services/*/docs/openapi.yaml"
            echo "This is acceptable for current project state"
            exit 0
          fi
          
          echo "Found OpenAPI specifications:"
          echo "$OPENAPI_SPECS"
          echo ""
          
          # Validate each found OpenAPI specification
          echo "$OPENAPI_SPECS" | while read -r spec; do
            if [ -f "$spec" ]; then
              service=$(basename $(dirname $(dirname "$spec")))
              echo "========================================"
              echo "Validating OpenAPI spec for $service: $spec"
              echo "========================================"
              
              # Basic structural validation with swagger-cli
              echo "Running swagger-cli validation..."
              if ! swagger-cli validate "$spec"; then
                echo "âŒ Swagger CLI validation failed for $spec"
                exit 1
              fi
              echo "âœ… Swagger CLI validation passed"
              
              # Lint with spectral (using built-in ruleset if custom .spectral.yml doesn't exist)
              echo "Running spectral linting..."
              if [ -f ".spectral.yml" ]; then
                echo "Using custom spectral ruleset: .spectral.yml"
                if ! spectral lint "$spec" --ruleset .spectral.yml --format stylish; then
                  echo "âŒ Spectral linting failed for $spec"
                  exit 1
                fi
              else
                echo "Using default spectral OpenAPI ruleset"
                if ! spectral lint "$spec" --format stylish; then
                  echo "âŒ Spectral linting failed for $spec"
                  exit 1
                fi
              fi
              echo "âœ… Spectral linting passed"
              echo ""
            fi
          done
          
          echo "âœ… All OpenAPI specifications validated successfully"
          
      - name: Run contract tests
        run: |
          # Start services in background for contract testing
          cargo build --workspace
          # Run contract validation tests
          cargo test contract --workspace --verbose
        env:
          DATABASE_URL: postgres://postgres:password@localhost:5432/gamalan_test

  # =============================================================================
  # BUILD VERIFICATION
  # =============================================================================
  
  build-verification:
    name: Build Verification
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [integration-tests, contract-tests]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Install Rust toolchain
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          override: true
          
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-build-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-build-
            ${{ runner.os }}-cargo-
            
      - name: Build API Gateway (consolidated services)
        run: |
          echo "Building consolidated API Gateway containing all services..."
          cargo build --release --package api-gateway
          echo "âœ… Consolidated API Gateway built successfully"
          echo "   - Includes: projects, backlog, readiness, prompt-builder, context-orchestrator services"
          echo "   - Single binary deployment: target/release/api-gateway"
        
      - name: Install pnpm
        uses: pnpm/action-setup@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'pnpm'
        
      - name: Build frontend
        working-directory: apps/web
        run: |
          pnpm install --frozen-lockfile
          pnpm run build
        env:
          # Use test environment variables from secrets
          NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY: ${{ secrets.TEST_CLERK_PUBLISHABLE_KEY }}
          CLERK_SECRET_KEY: ${{ secrets.TEST_CLERK_SECRET_KEY }}
          NEXT_PUBLIC_API_BASE_URL: http://localhost:8000
          
      - name: Test Docker builds
        run: |
          # Test that we can build Docker images
          echo "Testing Docker builds..."
          # For now just verify Dockerfiles exist
          find . -name "Dockerfile" -exec echo "Found Dockerfile: {}" \;

  # =============================================================================
  # SUMMARY & NOTIFICATIONS
  # =============================================================================
  
  pr-summary:
    name: PR Summary
    runs-on: ubuntu-latest
    if: always()
    needs: [static-analysis, frontend-static-analysis, backend-unit-tests, frontend-unit-tests, integration-tests, contract-tests, build-verification]
    
    steps:
      - name: PR Status Summary
        run: |
          echo "## ðŸš€ PR Validation Results" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.static-analysis.result }}" == "success" ]; then
            echo "âœ… Static Analysis: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Static Analysis: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.frontend-static-analysis.result }}" == "success" ]; then
            echo "âœ… Frontend Static Analysis: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Frontend Static Analysis: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.backend-unit-tests.result }}" == "success" ]; then
            echo "âœ… Backend Unit Tests: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Backend Unit Tests: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.frontend-unit-tests.result }}" == "success" ]; then
            echo "âœ… Frontend Unit Tests: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Frontend Unit Tests: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.integration-tests.result }}" == "success" ]; then
            echo "âœ… Integration Tests: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Integration Tests: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.contract-tests.result }}" == "success" ]; then
            echo "âœ… Contract Tests: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Contract Tests: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.build-verification.result }}" == "success" ]; then
            echo "âœ… Build Verification: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Build Verification: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Ready to merge:** All checks must pass" >> $GITHUB_STEP_SUMMARY